**1. Introduction**

This document outlines the Product Requirements for v0 of the AI Ad Agent, codenamed "Popmint" The primary goal of v0 is to rapidly develop a functional prototype to test core market assumptions and validate the user experience of an AI-powered ad creative generation tool. This version will focus on a streamlined user flow, essential features, and a specific tech stack to enable fast iteration.

**2. Goals for v0**

- Validate the core user flow: input product link/prompt -> receive AI-generated ad images on a canvas.
- Test the usability of the chat-based agent interaction model for ad creation.
- Assess the quality and relevance of AI-generated ad images.
- Deliver a working local-first application.

**3. Target Audience (for v0 testing)**

- Small business owners and marketers looking for quick ad creative solutions.
- Freelancers and agencies needing to generate ad variations rapidly.
- Individuals curious about AI's application in advertising.

**4. Product Overview**

Popmint v0 is a web application that allows users to generate ad creatives using AI. Users will input a product link and a text prompt. An AI agent will then process this information, interact with the user via a chat interface for clarification or steps, and generate ad images that are displayed on an interactive canvas.

**5. Key Features (v0)**

**5.1. Home Screen**

- **Input Fields:**
- Large text area for user prompt (e.g., "Create an ad for Diwali highlighting its freshness").
- Input field for product URL.
- (Implied from PRD, but not explicit) "Add Image" button (bottom left) - *Scope for v0: This button on the home screen will be deferred. Image input will primarily be via the chat panel in the playground.*
- **Action:**
- "Send" button (bottom right).
- **Visuals:**
- Top bar (minimalist, perhaps just app name/logo).
- Title and Subtitle.
- Suggestion pills (e.g., "Create a Facebook ad," "Generate Instagram story," "Focus on discounts") - *Scope for v0: Static, non-functional pills to show UI intent. Functionality deferred.*
- **Behavior:** Upon "Send," the user is redirected to the Agent Playground screen with the context passed.

**5.2. Agent Playground Screen**

- **5.2.1. Top Bar:**
    - **Extreme Left:** "Logo + Popmint" - clickable, navigates back to Home Screen.
    - **Center:** Project Name (e.g., "Whey Protein Campaign").
        - Clicking reveals a dropdown:
            1. Rename (renames current session's project name, local state only).
            2. Duplicate (not in v0 - too complex without persistence).
            3. Delete (not in v0 - too complex without persistence).
        - *Scope for v0: Rename updates a local state variable. No persistence.*
    - **Extreme Right:**
        - Zoom control dropdown for canvas (25%, 50%, 75%, 100%, Fit to Screen).
- **5.2.2. Canvas Area (Right Pane - 75% width):**
    - **Appearance:** Infinity canvas board (conceptually). White background (#FFFFFF), 10px border-radius, #000000 border with 5% opacity. (Max dimensions 1131px or 75% of width, whichever is smaller).
    - **Image Display:**
        - Images generated by the agent are automatically added to the canvas.
        - If agent returns multiple images (e.g., 5), they are added next to each other with 40px spacing.
        - Images maintain their actual aspect ratio and resolution from the agent.
    - **Canvas Controls (React Konva):**
        1. **Add Image from Agent:** Automatically handled.
        2. **Add Image (User Upload):** Via button in Chat Panel input area.
        3. **Zoom In/Out:** Using top-bar dropdown and pinch-to-zoom (if on touch device, otherwise scroll wheel). Steps of 10% for dropdown, smoother for pinch/scroll.
        4. **Fit to Screen:** Calculates bounding box of all elements and zooms/pans to fit.
        5. **Adding Editable Text:** Via a toolbar button (or right-click context menu - simpler for v0: toolbar button). Creates a default text node. Double-click text to edit.
        6. **Undo/Redo:** Buttons in a small floating toolbar or top bar. Manages a history of canvas states.
        7. **Delete Image/Text:** Select item, then press Cmd/Ctrl + Backspace or Delete key.
        8. **Panning:** Two-finger swipe on touch bar/trackpad, or click-and-drag canvas background.
        9. **Moving Image/Text:** Click and drag individual items.
        10. **Scaling Image/Text:** Select item, show transform handles to scale.
- **5.2.3. Chat Panel (Left Pane - 25% width or 372px):**
    - **Interaction Area:** Displays conversation between user and AI agent.
    - **Streaming:** Agent outputs, steps, and updates are streamed.
    - **Message Types & Structure:**
        - **User Input:** { type: "userInput", content: "User's text/link", timestamp: Date }
        - **Agent Request:** { type: "agentRequest", content: "Agent asks for clarification/approval", options?: ["Yes", "No"], timestamp: Date }
        - **Agent Progress:** { type: "agentProgress", content: "Thinking...", "Researching...", "Generating images...", details?: "Currently scraping product page", timestamp: Date } (Pattern: [Heart GIF] + message)
        - **Agent Handover (Internal State Change):** { type: "agentHandover", from_state: "research", to_state: "ad_concept", message: "Moving to ad concept phase.", timestamp: Date }
        - **Agent Output (Text):** { type: "agentOutput", sub_type: "text" | "ad_concept", content: "Generated text/concept.", timestamp: Date }
        - **Agent Output (Image Signal):** { type: "agentOutput", sub_type: "image_generated", imageUrls: ["url1", "url2"], message: "Here are the images I've generated.", timestamp: Date } (Backend sends this, frontend then adds to canvas)
        - **Agent Update (Completion/Status):** { type: "agentUpdate", status: "completed" | "error", message: "Task completes in X minutes" or "Error message", duration?: "X minutes", timestamp: Date }
    - **Chat Input Area (Bottom of Chat Panel):**
        - Text input field.
        - Bottom Left: "Attach Image" button (for user to upload an image as context).
        - Bottom Right: "Send" button.

**6. Tech Stack**

- **Frontend:**
    - Framework: Next.js (App Router)
    - Language: TypeScript
    - Styling: Tailwind CSS
    - UI Components: Shadcn/ui
    - Canvas: React Konva
    - State Management: Zustand (lightweight, good for v0)
    - Icons: Lucide Icons (comes with Shadcn/ui) or I will provide the Icon SVG
- **Backend (Agent Orchestration):**
    - Language: Python 3.9+
    - Framework: FastAPI (for API endpoints and SSE)
    - Agent SDK: OpenAI Agent SDK
    - LLM: OpenAI (via Agent SDK)
    - Image Generation: "GPT Image" (assumed to be DALL-E or similar accessible via OpenAI SDK or a custom tool within the agent)
    - Data Validation: Pydantic
- **Communication:**
    - Client-Server: HTTPS (RESTful API calls for initial request, SSE for streaming chat)
- **Development:**
    - Package Manager: npm/yarn (Frontend), pip/poetry (Backend)
    - Version Control: Git

**7. Architecture Overview**

```python
      +---------------------+      HTTP/S      +------------------------+     OpenAI API      +-----------------+
|    User Browser     |<---------------->|   Next.js Frontend     |<------------------->| OpenAI Services |
| (React, Konva, CSS) |                  | (API Routes for Proxy) |                     | (LLM, DALL-E)   |
+---------------------+                  +------------------------+                     +-----------------+
      ^      |                                    |        ^
      |      | SSE (Chat Stream)                    |        | HTTP/S
      |      |                                    V        | (Agent Actions)
      |      +------------------------------------+--------+-----------------+
      |                                           |  Backend (Python/FastAPI) |
      +-------------------------------------------+ (OpenAI Agent SDK)       |
        (User Actions, Initial Prompt)            | (Agent Orchestration,    |
                                                  |  Image URL processing)    |
                                                  +---------------------------+
```

**Data Flow (Simplified):**

1. User submits prompt + link on Home Screen (FE).
2. FE sends POST request to Next.js API route (/api/agent/start).
3. Next.js API route forwards request to FastAPI backend (/start-session).
4. FastAPI backend initializes OpenAI Agent with the prompt.
5. FE, after redirection to Playground, establishes an SSE connection to a Next.js API route (/api/agent/stream/[sessionId]) which proxies to FastAPI (/stream-events/[sessionId]).
6. FastAPI backend streams agent events (progress, messages, image URLs) over SSE.
7. FE receives streamed events:
    - Chat messages update the Chat Panel.
    - Image URLs are used to load images onto the React Konva canvas.
8. User interactions in chat (sending messages, approving actions) go via POST to Next.js API route (/api/agent/message) proxying to FastAPI (/post-message/[sessionId]).
9. Canvas interactions are local to the FE but can trigger new agent requests if designed (e.g., "regenerate this image").

**8. Frontend Implementation Details**

**8.1. File Structure (Next.js App Router - illustrative)**

```python
      /app
  /api
    /agent
      /start/route.ts        // POST: Start agent session
      /stream/[sessionId]/route.ts // GET: SSE chat stream
      /message/route.ts      // POST: Send message to agent
  /(app)
    /layout.tsx
    /page.tsx                // Home Screen
    /playground/[sessionId]
      /page.tsx              // Agent Playground Screen
      /components/
        TopBar.tsx
        ChatPanel.tsx
          MessageBubble.tsx
          ChatInput.tsx
        CanvasArea.tsx         // Manages Konva Stage & Layers
          KonvaImage.tsx
          KonvaText.tsx
          CanvasToolbar.tsx
  /components/ui/            // Shadcn components
  /lib/                      // Utilities, Konva helpers
  /hooks/                    // Custom hooks (e.g., useKonvaUndoRedo)
  /store/                    // Zustand store
    canvasStore.ts
    chatStore.ts
    sessionStore.ts
public/
  logo.svg
  heart.gif
```

**8.2. Key Components & Logic**

- **app/page.tsx (Home Screen):**
    - Form with textarea and input.
    - On submit, POST to /api/agent/start, get back a sessionId, then router.push(/playground/[sessionId]).
- **app/playground/[sessionId]/page.tsx (Playground):**
    - Main layout component splitting into ChatPanel and CanvasArea.
    - Fetches sessionId from params.
    - Initializes SSE connection to /api/agent/stream/[sessionId].
    - Manages overall playground state (project name).
- **ChatPanel.tsx:**
    - Displays messages from chatStore.
    - ChatInput.tsx for sending messages (POST to /api/agent/message).
    - Handles display of different message types (styling, Heart GIF).
- **CanvasArea.tsx:**
    - Renders <Stage> from React Konva.
    - Manages layers for images and text.
    - Listens to canvasStore for new images/text to add.
    - Implements pan, zoom (from top bar), fit-to-screen.
    - Handles selection and deletion of objects.
    - Integrates CanvasToolbar.tsx for undo/redo, add text.
- **KonvaImage.tsx, KonvaText.tsx:**
    - Wrapper components for Konva shapes.
    - Implement draggable, scalable (with transformer).
    - Text component handles double-click to edit (overlay HTML input).
- **State Management (Zustand):**
    - sessionStore: sessionId, projectName.
    - chatStore: messages: Array<ChatMessage>, agentStatus: string.
    - canvasStore: objects: Array<KonvaObject>, history: Array<CanvasState>, historyStep: number, selectedObjectId: string | null, zoomLevel: number.
        - KonvaObject: { id: string, type: 'image' | 'text', x: number, y: number, width?: number, height?: number, src?: string, text?: string, ...konvaProps }
        - CanvasState: A snapshot of all objects (JSON serializable).

**8.3. React Konva Implementation Notes:**

- **Stage Setup:** Set initial width/height. draggable for panning.
- **Layers:** Use at least one main layer for ad elements.
- **Images:** Konva.Image. Use window.Image to load image from URL, then pass to Konva.
- **Text:** Konva.Text. For editing, on dblclick, show an absolute positioned HTML <textarea> over the Konva text, update Konva text on blur/enter.
- **Selection:** Click handler on shapes to update selectedObjectId in store. Use Konva.Transformer for selected items.
- **Zoom:** stageRef.current.scale({ x: newZoom, y: newZoom }). Adjust stageRef.current.position() to zoom towards center or mouse pointer.
- **Fit to Screen:**
    
    ```python
          const stage = stageRef.current;
    const nodes = layerRef.current.children;
    if (!nodes || nodes.length === 0) return;
    let minX = Infinity, minY = Infinity, maxX = -Infinity, maxY = -Infinity;
    nodes.forEach(node => {
        const box = node.getClientRect(); // consider transformer size too if active
        minX = Math.min(minX, box.x);
        minY = Math.min(minY, box.y);
        maxX = Math.max(maxX, box.x + box.width);
        maxY = Math.max(maxY, box.y + box.height);
    });
    const boundsWidth = maxX - minX;
    const boundsHeight = maxY - minY;
    if (boundsWidth === 0 || boundsHeight === 0) return; // Avoid division by zero
    
    const scaleX = stage.width() / boundsWidth;
    const scaleY = stage.height() / boundsHeight;
    const scale = Math.min(scaleX, scaleY) * 0.9; // 0.9 for some padding
    
    stage.scale({ x: scale, y: scale });
    stage.position({
        x: -minX * scale + (stage.width() - boundsWidth * scale) / 2,
        y: -minY * scale + (stage.height() - boundsHeight * scale) / 2,
    });
    stage.batchDraw();
    ```
    
- **Undo/Redo:**
    - Store an array of canvas states (JSON representation of all objects and their properties).
    - On any change that should be undoable, push the new state to the history array.
    - Undo: Go to history[currentStep - 1]. Redo: Go to history[currentStep + 1].
    - Clear future history if a new action is taken after undoing.
- **Keyboard Shortcuts:** Global event listener (useEffect in CanvasArea) for Cmd/Ctrl + Backspace/Delete.

**9. Backend Implementation Details (Python/FastAPI & OpenAI Agent SDK)**

**9.1. File Structure (Illustrative)**

```python
      /popmint_backend
  /app
    main.py                 // FastAPI app, router setup
    /api
      endpoints.py          // API endpoint definitions
    /agents
      ad_creation_agent.py  // Agent definition, tools
      event_handler.py      // Custom AssistantEventHandler
    /services
      stream_manager.py     // Manages active SSE streams
    /models
      schemas.py            // Pydantic models for API requests/responses
  requirements.txt
  .env
```

**9.2. API Endpoints (endpoints.py)**

- **POST /start-session**:
    - Request Body: { prompt: string, product_url: string } (validated by Pydantic).
    - Creates an OpenAI Assistant client.
    - Creates a new Thread.
    - Adds user's initial message (prompt + URL context) to the Thread.
    - Creates a Run on the Thread with the configured Assistant.
    - Generates a unique sessionId.
    - Stores thread_id and run_id associated with sessionId (simple in-memory dict for v0).
    - Returns { sessionId: string }.
- **GET /stream-events/{session_id}**:
    - SSE endpoint.
    - Retrieves thread_id for session_id.
    - Uses client.beta.threads.runs.stream(thread_id=..., assistant_id=..., event_handler=CustomEventHandler(sse_queue))
    - The CustomEventHandler will put formatted messages (see 5.2.3) onto an SSE queue specific to this session_id.
    - The endpoint yields messages from this queue.
- **POST /post-message/{session_id}**:
    - Request Body: { content: string, image_url?: string }
    - Retrieves thread_id.
    - Adds user's message to the Thread: client.beta.threads.messages.create(...).
    - Creates a new Run: client.beta.threads.runs.create_and_stream(...) or client.beta.threads.runs.create(...) followed by streaming, to process the new message.
    - Returns { success: true }.

**9.3. Agent Orchestration (ad_creation_agent.py, event_handler.py)**

- **CustomEventHandler(StreamingResponseQueue) (from openai.lib.streaming import AssistantEventHandler):**
    
    ```python
         from openai.lib.streaming import AssistantEventHandler
    from typing_extensions import override
    import time
    # from app.services.stream_manager import sse_manager # For putting events to a queue
    
    class CustomEventHandler(AssistantEventHandler):
        def __init__(self, session_id):
            super().__init__()
            self.session_id = session_id # To use with sse_manager
    
        def _format_message(self, type, data):
            # This structure should match 5.2.3 Chat Message Types
            return {"type": type, "timestamp": time.time(), **data}
    
        @override
        def on_text_created(self, text) -> None:
            # print(f"\nassistant > ", end="", flush=True)
            # For v0, we might just focus on on_text_delta for streaming
            # sse_manager.put_event(self.session_id, self._format_message("agentProgress", {"content": "Agent creating text..."}))
            pass
    
        @override
        def on_text_delta(self, delta, snapshot):
            # print(delta.value, end="", flush=True)
            # This is where agent's text output streams
            # For simplicity, might send full snapshot.text.value on each delta
            # or accumulate and send periodically.
            # For this PRD, let's assume we send the delta.value
            sse_manager.put_event(self.session_id, self._format_message("agentOutput", {"sub_type": "text_delta", "content": delta.value}))
    
        @override
        def on_tool_call_created(self, tool_call):
            # print(f"\nassistant > {tool_call.type}\n", flush=True)
            # Potentially map to agentRequest or agentProgress
            sse_manager.put_event(self.session_id, self._format_message("agentProgress", {"content": f"Starting tool: {tool_call.type}"}))
    
        @override
        def on_tool_call_delta(self, delta, snapshot):
            # if delta.type == 'code_interpreter':
            #     # ... handle code interpreter ...
            # elif delta.type == 'function':
            #     # print(delta.function.arguments, end="", flush=True)
            #     pass # Function arguments are streamed
            pass # More granular progress
    
        # --- Mapping to PRD Chat Structure ---
        # thread.run.created -> agentProgress "Run started"
        # thread.run.queued -> agentProgress "Run queued"
        # thread.run.in_progress -> agentProgress (generic or based on step type)
        # thread.run.requires_action -> agentRequest (if tool needs user input, or map tool outputs to this)
        # thread.run.step.created -> agentProgress (e.g., "Starting step: Research")
        # thread.run.step.in_progress -> agentProgress (more details)
        # thread.run.step.delta -> if it contains messages, map to agentOutput
        # thread.run.step.completed -> agentProgress or agentHandover
        # thread.message.created / thread.message.in_progress / thread.message.delta -> agentOutput (text)
        # thread.message.completed -> agentOutput (final message state)
        # thread.run.completed -> agentUpdate "Task completed"
        # thread.run.failed / thread.run.cancelled / thread.run.expired -> agentUpdate "Error/Cancelled"
    
        # Example of how to handle specific events from the SDK to match your chat types:
        @override
        def on_event(self, event):
            if event.event == 'thread.run.step.created':
                step_details = event.data.step_details
                step_type = "Unknown step"
                if step_details.type == 'tool_calls':
                    # Example: Assuming first tool call gives type
                    tool_name = step_details.tool_calls[0].function.name if step_details.tool_calls else "tool"
                    step_type = f"Executing tool: {tool_name}"
                elif step_details.type == 'message_creation':
                    step_type = "Composing message"
                sse_manager.put_event(self.session_id, self._format_message("agentProgress", {"content": step_type}))
    
            elif event.event == 'thread.message.delta':
                 # Already handled by on_text_delta if using that.
                 # If not, then:
                 # delta_content = event.data.delta.content[0]
                 # if delta_content.type == "text":
                 #    sse_manager.put_event(self.session_id, self._format_message("agentOutput", {"sub_type": "text_delta", "content": delta_content.text.value}))
                 pass
    
            # If agent uses a tool to generate an image and the tool output is the image URL(s)
            if event.event == 'thread.run.step.completed' and event.data.step_details.type == 'tool_calls':
                for tool_call in event.data.step_details.tool_calls:
                    if tool_call.function.name == "generate_ad_image": # Assuming such a tool
                        # The output of the tool call (image URL) would have been submitted by you
                        # in a previous client.beta.threads.runs.submit_tool_outputs_stream call.
                        # This event just confirms the step is done.
                        # Actual image URL emission should happen when the tool *outputs* it to the agent,
                        # or the agent messages it.
                        # For this v0, let's assume the agent sends a regular message with the image URL.
                        # A more robust way is a dedicated event type.
                        # If the tool_call.output contains the image URL (after you submit it):
                        # image_urls_json = tool_call.output 
                        # try:
                        #    image_data = json.loads(image_urls_json)
                        #    if "imageUrls" in image_data:
                        #        sse_manager.put_event(self.session_id, self._format_message("agentOutput", {"sub_type": "image_generated", "imageUrls": image_data["imageUrls"]}))
                        # except: pass
                        pass
    
            if event.event == 'thread.run.completed':
                sse_manager.put_event(self.session_id, self._format_message("agentUpdate", {"status": "completed", "message": "Task completed."}))
            elif event.event in ['thread.run.failed', 'thread.run.cancelled', 'thread.run.expired']:
                 sse_manager.put_event(self.session_id, self._format_message("agentUpdate", {"status": "error", "message": f"Run {event.event}."}))
    ```
    

- **Agent Definition (ad_creation_agent.py):**
    - Define instructions for the assistant (e.g., "You are an expert ad creator...").
    - Define tools:
        - product_scraper: A function tool to fetch and parse content from product_url.
        - image_generator: A function tool that takes a detailed prompt and returns image URL(s) (e.g., by calling DALL-E API). *The agent will decide when to call this.*
        - The output of these tools will be submitted back to the run using client.beta.threads.runs.submit_tool_outputs_stream.
- **Image Generation:** When the image_generator tool is called by the agent, its output (the image URLs) needs to be sent to the frontend. The agent might explicitly "say" "Here are your images: [URL1], [URL2]" which streams via on_text_delta. The CustomEventHandler could parse these messages for image URLs or the backend can have a specific event for it. For v0, parsing a specific message format from the agent is simplest. A structured agentOutput with sub_type: "image_generated" is cleaner.

**9.4. Chat Streaming Logic Mapping:**

(Refer to CustomEventHandler above for direct mapping examples)

- **User Input:** Handled by FE sending POST.
- **Agent Request:** event.event == 'thread.run.requires_action' if the required_action involves a tool that needs explicit user approval before execution. Or, agent can send a text message asking for approval.
- **Agent Progress:** thread.run.step.created, thread.run.step.in_progress. Can be augmented with custom messages from tools (e.g., "Scraping product page..."). Heart GIF added by frontend.
- **Agent Handover:** Can be inferred from thread.run.step.completed for one tool and thread.run.step.created for the next, or agent can explicitly state it in a message.
- **Agent Output (Text/Image):** thread.message.delta for text. For images, if a tool generates an image URL, the agent needs to output this URL in a message, or the backend can intercept the tool's output. The agentOutput type with sub_type: "image_generated" is preferred.
- **Agent Updates:** thread.run.completed, thread.run.failed, etc.

**10. Database/Storage (v0)**

- **None.** All state is ephemeral or managed client-side (e.g., canvas state in localStorage for very basic persistence if desired, but not a core v0 requirement). Project "rename" is purely cosmetic in the UI for the current session.

**11. Deployment (v0 - For testing)**

- **Frontend (Next.js):** Vercel (ideal for Next.js).
- **Backend (FastAPI):** Render, Fly.io, or even a simple Docker container on a cloud VM.
- For pure local development, npm run dev and uvicorn main:app --reload.

**12. Non-Goals for v0**

- User authentication and accounts.
- Persistent storage of projects or generated ads.
- Advanced collaboration features.
- Billing or subscription management.
- Complex CRUD operations for projects (duplicate, delete across sessions).
- Multi-language support.
- Responsive design beyond basic desktop usability.
- Suggestion pill functionality.
- Advanced AI agent capabilities beyond core ad image generation.
- "Add Image" button on the Home Screen.

**13. Future Considerations (Post-v0 / Windsurf IDE)**

- Componentizing canvas elements for reusability in Windsurf.
- API design for state persistence.
- User accounts and data storage.
- More sophisticated agent interaction flows.